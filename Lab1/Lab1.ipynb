{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import time\n",
    "\n",
    "first_url = 'https://bigenc.ru/c/chiornaia-metallurgiia-97d762/'\n",
    "base_url = 'https://bigenc.ru'\n",
    "\n",
    "\n",
    "# хранение связей url\n",
    "# title : {[[url], [annotation], [additional_title_list]] ... }\n",
    "main_dict = dict()\n",
    "second_iteration = dict()\n",
    "# отработанные URL \n",
    "thrift_box = []\n",
    "\n",
    "\n",
    "blacklist = ['http', \n",
    "            'versions',\n",
    "            'references',\n",
    "            'media',\n",
    "            'about-project',\n",
    "            'partners',\n",
    "            'copyright-holders',\n",
    "            'contacts',\n",
    "            'tel:',\n",
    "            'mailto:',\n",
    "            'auth',\n",
    "            'annotation',\n",
    "            '/t/',\n",
    "            '/l/',\n",
    "            '/a/']\n",
    "\n",
    "# Проверка наличия в полученном url слов из blacklist\n",
    "def check_blacklist(link):\n",
    "    for word in blacklist:\n",
    "        if word in link or link == '/':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Проверка наличия контента на странице (много пустых гиперссылок)\n",
    "def check_url(url):\n",
    "    child_dict = dict()\n",
    "    response = requests.get(base_url+url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if soup.find('h1', '-text-headline-1-small-x2 md:-text-headline-1-small tw-m-0') == None:\n",
    "        title = soup.find('h1', 'bre-article-header-title').text\n",
    "        annotation = get_annotation(response)\n",
    "        child_dict[title] = {'annotation' : annotation,\n",
    "                            'url' : base_url + url,\n",
    "                            'additional_title' : []}\n",
    "        return child_dict, title\n",
    "    return None, None\n",
    "\n",
    "# Парсим аннотацию, без XPATH обойтись не удалось, селекторы на страницах отличаются\n",
    "def get_annotation(response):\n",
    "    # загружаем HTML как объект lxml\n",
    "    page = html.fromstring(response.text) \n",
    "    element = page.xpath('/html/body/div[1]/div[2]/main/div[1]/article/div[3]/div[2]/div[1]/div/div/section/section/p[1]')[0]\n",
    "    annotation = element.text_content()\n",
    "    return annotation\n",
    "\n",
    "def main(url):\n",
    "    work_dict = dict()\n",
    "    if url in thrift_box:\n",
    "        return None\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup.find('h1', 'bre-article-header-title').text\n",
    "    annotation = get_annotation(response)\n",
    "    work_dict[title] = {'annotation' : annotation,\n",
    "                        'url' : url,\n",
    "                        'additional_title' : []}\n",
    "    links = soup.find('body').find_all('a')\n",
    "    for link in links:\n",
    "        if len(work_dict[title]['additional_title']) >= 10:\n",
    "            break\n",
    "        additional_url = str(link.get('href'))\n",
    "        if check_blacklist(additional_url):\n",
    "            child_dict, child_title = check_url(additional_url)\n",
    "            if child_dict != None:\n",
    "                work_dict.update(child_dict)\n",
    "                work_dict[title]['additional_title'].append(child_title)\n",
    "    thrift_box.append(url)\n",
    "    return work_dict\n",
    "\n",
    "        \n",
    "main_dict = main(first_url)\n",
    "second_iteration = main_dict.copy()\n",
    "\n",
    "for i in main_dict.keys():\n",
    "    nw = (main(main_dict[i]['url']))\n",
    "    if nw:\n",
    "        second_iteration.update(nw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(main_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph.pdf'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz as gv\n",
    "\n",
    "g = gv.Graph()\n",
    "\n",
    "\n",
    "# Добавляем ребра для ключей в additional_title\n",
    "for key, val in second_iteration.items():\n",
    "    if 'additional_title' in val:\n",
    "        for title in val['additional_title']:\n",
    "            g.edge(key, title)\n",
    "\n",
    "\n",
    "g.render('graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'w', encoding='utf-8') as f:\n",
    "  for key, value in second_iteration.items():\n",
    "    f.write(f'Title: {key}\\n')\n",
    "    f.write(f'Url: {value[\"url\"]}\\n') \n",
    "    f.write(f'Annotation: {value[\"annotation\"]}\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScraperV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
